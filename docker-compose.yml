# üêö Paguro - Docker Compose per Villa Celi
# Setup: API Paguro + Ollama per Palinuro, Cilento

services:
  # üêö Paguro API - Receptionist Virtuale
  paguro-api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: paguro-api-simple
    restart: unless-stopped
    ports:
      - "5000:5000"
    environment:
      - FLASK_ENV=development
      - DB_PATH=/app/data/affitti2025.db
      - OLLAMA_URL=http://ollama:11434/api/generate
      - MODEL=llama3.2:1b
      - PORT=5000
      - PYTHONUNBUFFERED=1
      - HOST=0.0.0.0
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
      - ./cache:/app/cache
    depends_on:
      - ollama
    networks:
      - paguro-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # ü§ñ Ollama - AI Engine per Paguro
  ollama:
    build:
      context: .
      dockerfile: ollama.Dockerfile
    container_name: paguro-ollama-simple
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    environment:
      - OLLAMA_MODELS=llama3.2:1b
      - OLLAMA_GPU=false
      - CUDA_VISIBLE_DEVICES=""
    # Rimuovi la riga 'command: ollama pull llama3.2:1b' da qui
    networks:
      - paguro-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/version"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    # Decommentare solo se hai GPU NVIDIA
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

# üì° Network per comunicazione interna Paguro
networks:
  paguro-network:
    driver: bridge
    name: paguro-network

# üíæ Volumi persistenti per Paguro
volumes:
  ollama-data:
    name: paguro-ollama-data
    driver: local
